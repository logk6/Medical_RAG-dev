{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "File này là để test các hàm, show dữ liệu trực quan",
   "id": "db14a8efc3b1f692"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-08T09:28:37.390125Z",
     "start_time": "2025-11-08T09:28:36.972493Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:8: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\Dung\\AppData\\Local\\Temp\\ipykernel_2292\\4119271182.py:8: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  pdf_paths = [\"D:\\DUNG\\MasterProgram\\Project_I\\Medical_RAG\\medical_RAG_system\\data\\pdf_document\\DiffWater_UIE.pdf\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a84d136f87434f17b42691f1b331edb5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf89c581e603421aa76f5603628ad984"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "45d3bf5e846140e79d0cb2d680af1d7e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "62764cf22f9c46be815364241abffb3b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a081251c55cc4cae835b2239b7e58c83"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 28,
   "source": [
    "import fitz\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "pdf_paths = [\"D:\\DUNG\\MasterProgram\\Project_I\\Medical_RAG\\medical_RAG_system\\data\\pdf_document\\DiffWater_UIE.pdf\"]\n",
    "def text_formatter(text: str) -> str:\n",
    "    \"\"\"Xử lí clean text.\"\"\"\n",
    "    cleaned_text = text.replace(\"\\n\", \" \").strip()\n",
    "    return cleaned_text\n",
    "\n",
    "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages_and_texts = []\n",
    "    for page_number, page in tqdm(enumerate(doc)):  # lặp từng trang trong doc\n",
    "        text = page.get_text()  # Lấy text\n",
    "        text = text_formatter(text) # Clean text\n",
    "        pages_and_texts.append({\"page_number\": page_number,\n",
    "                                \"page_char_count\": len(text),\n",
    "                                \"page_word_count\": len(text.split(\" \")),\n",
    "                                \"page_sentence_count_raw\": len(text.split(\". \")),\n",
    "                                \"page_token_count\": len(text) / 4,  # 1 token = ~4 chars\n",
    "                                \"text\": text})\n",
    "    return pages_and_texts\n",
    "\n",
    "for idx, pdf_path in enumerate(tqdm(pdf_paths)):\n",
    "    pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)\n",
    "\n",
    "\n",
    "    from spacy.lang.en import English\n",
    "    nlp = English()\n",
    "    nlp.add_pipe(\"sentencizer\")\n",
    "    for item in tqdm(pages_and_texts):\n",
    "        item[\"sentences\"] = list(nlp(item[\"text\"]).sents) # Biến giá trị trong text thành list các câu\n",
    "        item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]] # Đảm bảo mỗi câu đều là str\n",
    "        item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])\n",
    "\n",
    "    num_sentence_chunk_size = 10\n",
    "    def split_list(input_list: list, slice_size: int) -> list[list[str]]:\n",
    "        return [input_list[i:i + slice_size] for i in range(0, len(input_list), slice_size)]\n",
    "    \"\"\"Phải trả về list của các list[str] vì sau đó sẽ có bước tách mỗi chunk là 1 list[str].\n",
    "       Cách tách: mỗi 10 câu sẽ thành 1 list. VD lúc đầu sentence có 17 câu, thì sẽ tách thành 1 list 10 câu và 1 list 7 câu: [ [10], [7] ]. \"\"\"\n",
    "\n",
    "    for item in tqdm(pages_and_texts):\n",
    "        item[\"sentence_chunks\"] = split_list(input_list=item[\"sentences\"],\n",
    "                                             slice_size=num_sentence_chunk_size)\n",
    "        item[\"num_chunks\"] = len(item[\"sentence_chunks\"])\n",
    "\n",
    "    import re\n",
    "\n",
    "    # Tách mỗi chunk thành 1 item riêng\n",
    "    pages_and_chunks = []\n",
    "    for item in tqdm(pages_and_texts):\n",
    "        for sentence_chunk in item[\"sentence_chunks\"]:\n",
    "            chunk_dict = {}\n",
    "            chunk_dict[\"page_number\"] = item[\"page_number\"]\n",
    "\n",
    "            # Trước đó thì mỗi chunk là 1 list chứa nhiều câu. Bây giờ sẽ ghép lại thành 1 câu, ko còn list nữa\n",
    "            joined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").strip()\n",
    "            joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk) # \".A\" -> \". A\" for any full-stop/capital letter combo\n",
    "            chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n",
    "\n",
    "            # Get stats about the chunk\n",
    "            chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
    "            chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n",
    "            chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4 # 1 token = ~4 characters\n",
    "\n",
    "            pages_and_chunks.append(chunk_dict)\n",
    "\n",
    "    # Lấy ra 1 list tất cả text, mỗi phần từ là 1 chunk, mỗi chunk là 1 string đã được xử lý.\n",
    "    # text_chunks = [item[\"sentence_chunk\"] for item in pages_and_chunks]\n",
    "    file_stem = Path(pdf_path).stem\n",
    "    for c_idx, item in enumerate(pages_and_chunks):\n",
    "        data_chunked = {\n",
    "            \"id\": \"doc_\" + str(idx) + \"_chunk_\" + str(c_idx),\n",
    "            \"title\": str(file_stem),\n",
    "            \"text_chunked\": item[\"sentence_chunk\"],\n",
    "        }\n",
    "        source_text_chunked = Path('../../data/embed_data/source/text_chunked.jsonl')\n",
    "        with open(source_text_chunked, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(data_chunked, ensure_ascii=False) + \"\\n\")"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "6b41f966e305e048"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
